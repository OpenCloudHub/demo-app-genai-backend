name: Evaluate and Promote RAG Prompts

on:
  workflow_dispatch:
    inputs:
      prompt_name: 
        description: 'Prompt name to use'
        required: true
        default: 'readme-rag-prompt'
      prompt_versions:
        description: 'Prompt versions to evaluate (space-separated, e.g., "1 2 3")'
        required: true
        default: '1 2 3'
      data_version:
        description: 'Eval dataset version (DVC tag)'
        required: true
        default: 'opencloudhub-readmes-rag-evaluation-v1.0.0'
      auto_promote:
        description: 'Auto-promote best prompt to production'
        type: boolean
        default: true
      image_tag:
        description: 'Docker image tag'
        required: false
        default: 'latest-eval'

jobs:
  evaluate:
    name: ðŸ§ª Evaluate Prompts
    runs-on: self-hosted-local
    outputs:
      best_prompt_version: ${{ steps.get-result.outputs.best_prompt_version }}

    steps:
      - name: ðŸ”§ Install kubectl
        run: |
          if [ ! -f ~/.local/bin/kubectl ]; then
            curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            chmod +x kubectl
            mkdir -p ~/.local/bin
            mv kubectl ~/.local/bin/
          fi
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: ðŸ” Configure kubectl
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > ~/.kube/config
          kubectl cluster-info

      - name: ðŸ§ª Submit Evaluation Job
        run: |
          DOCKER_REPO="${{ secrets.DOCKER_USERNAME }}/demo-app-genai-backend"
          IMAGE_TAG="${{ inputs.image_tag }}"
          
          # Ensure -eval suffix
          if [[ "$IMAGE_TAG" != *-eval ]]; then
            IMAGE_TAG="${IMAGE_TAG}-eval"
          fi
          
          FULL_IMAGE="${DOCKER_REPO}:${IMAGE_TAG}"
          echo "ðŸ“¦ Using image: ${FULL_IMAGE}"

          cat <<EOF | kubectl create -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            generateName: rag-eval-
            namespace: mlops
            labels:
              pipeline: rag-evaluation
          spec:
            ttlSecondsAfterFinished: 600
            template:
              spec:
                restartPolicy: Never
                containers:
                  - name: eval
                    image: ${FULL_IMAGE}
                    command: ["python"]
                    args:
                      - "/workspace/project/src/evaluation/evaluate_promts.py"
                      - "--prompt-name"
                      - "${{ inputs.prompt_name }}"
                      - "--prompt-versions"
                      - "${{ inputs.prompt_versions }}"
                      - "--data-version"
                      - "${{ inputs.data_version }}"
                      ${{ inputs.auto_promote == true && '- "--auto-promote"' || '' }}
                    env:
                      - name: DB_CONNECTION_STRING
                        valueFrom:
                          secretKeyRef:
                            name: demo-app-db-user
                            key: connection-string
                      - name: AWS_ACCESS_KEY_ID
                        valueFrom:
                          secretKeyRef:
                            name: minio-tenant-secret
                            key: accesskey
                      - name: AWS_SECRET_ACCESS_KEY
                        valueFrom:
                          secretKeyRef:
                            name: minio-tenant-secret
                            key: secretkey
                      - name: AWS_ENDPOINT_URL
                        value: "https://minio-api.internal.opencloudhub.org"
                      - name: MLFLOW_TRACKING_URI
                        value: "http://mlflow.mlops.svc.cluster.local:80"
                      - name: MLFLOW_TRACKING_INSECURE_TLS
                        value: "true"
                      - name: LLM_BASE_URL
                        value: "http://qwen-serve.models.svc.cluster.local:8000/v1"
                      - name: DOCKER_IMAGE_TAG
                        value: "${IMAGE_TAG}"
                    resources:
                      requests:
                        cpu: "500m"
                        memory: "1Gi"
                      limits:
                        cpu: "2"
                        memory: "4Gi"
          EOF

      - name: â³ Wait for Evaluation Job
        id: wait
        run: |
          sleep 3
          JOB_NAME=$(kubectl get jobs -n mlops -l pipeline=rag-evaluation --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-1].metadata.name}')
          echo "ðŸ“‹ Job: $JOB_NAME"
          echo "job_name=$JOB_NAME" >> $GITHUB_OUTPUT

          kubectl wait --for=condition=complete --timeout=30m job/$JOB_NAME -n mlops || {
            echo "âŒ Evaluation job failed"
            kubectl logs -n mlops job/$JOB_NAME --tail=200
            exit 1
          }

          echo "âœ… Evaluation complete"

      - name: ðŸ“Š Get Results
        id: get-result
        run: |
          JOB_NAME="${{ steps.wait.outputs.job_name }}"
          
          # Get logs and extract best version
          kubectl logs -n mlops job/$JOB_NAME > /tmp/eval_output.txt
          cat /tmp/eval_output.txt
          
          BEST_VERSION=$(grep "Best prompt version:" /tmp/eval_output.txt | grep -oP 'v\K\d+' | head -1)
          
          if [ -z "$BEST_VERSION" ]; then
            echo "âš ï¸ Could not extract best prompt version from logs"
            BEST_VERSION="unknown"
          fi
          
          echo "âœ… Best prompt version: ${BEST_VERSION}"
          echo "best_prompt_version=${BEST_VERSION}" >> $GITHUB_OUTPUT

  reload-api:
    name: ðŸ”„ Reload Production API
    needs: evaluate
    if: inputs.auto_promote == true && needs.evaluate.outputs.best_prompt_version != '' && needs.evaluate.outputs.best_prompt_version != 'unknown'
    runs-on: self-hosted-local

    steps:
      - name: ðŸ”§ Install kubectl
        run: |
          if [ ! -f ~/.local/bin/kubectl ]; then
            curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            chmod +x kubectl
            mkdir -p ~/.local/bin
            mv kubectl ~/.local/bin/
          fi
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: ðŸ” Configure kubectl
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > ~/.kube/config

      - name: ðŸ”„ Trigger API Reload
        run: |
          BEST_VERSION="${{ needs.evaluate.outputs.best_prompt_version }}"
          
          # Call the API reload endpoint via kubectl exec or port-forward
          kubectl run reload-prompt --rm -i --restart=Never -n example-app \
            --image=curlimages/curl:latest -- \
            curl -s -X POST \
              "http://demo-app-backend.example-app.svc.cluster.local:8000/admin/reload-prompt" \
              -H "Content-Type: application/json" \
              -d "{\"prompt_version\":${BEST_VERSION}}"
          
          echo "âœ… API reload triggered for prompt v${BEST_VERSION}"

  summary:
    name: ðŸ“‹ Summary
    needs: [evaluate, reload-api]
    if: always()
    runs-on: self-hosted-local

    steps:
      - name: Create Job Summary
        run: |
          cat >> $GITHUB_STEP_SUMMARY << EOF
          ## ðŸŽ¯ RAG Prompt Evaluation Results
          
          ### Configuration
          | Parameter | Value |
          |-----------|-------|
          | **Prompt Name** | \`${{ inputs.prompt_name }}\` |
          | **Versions Evaluated** | \`${{ inputs.prompt_versions }}\` |
          | **Dataset Version** | \`${{ inputs.data_version }}\` |
          | **Auto-Promote** | ${{ inputs.auto_promote }} |
          | **Image Tag** | \`${{ inputs.image_tag }}\` |
          
          ### Results
          | Metric | Value |
          |--------|-------|
          | **Best Prompt Version** | \`v${{ needs.evaluate.outputs.best_prompt_version }}\` |
          | **Evaluation Status** | ${{ needs.evaluate.result == 'success' && 'âœ… Success' || 'âŒ Failed' }} |
          | **API Reload Status** | ${{ needs.reload-api.result == 'success' && 'âœ… Success' || needs.reload-api.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |
          EOF