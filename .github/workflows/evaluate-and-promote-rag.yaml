name: Evaluate and Promote RAG Prompts

on:
  workflow_dispatch:
    inputs:
      prompt_name: 
        description: 'Prompt name to use'
        required: true
        default: 'readme-rag-prompt'
      prompt_versions:
        description: 'Prompt versions to evaluate (space-separated, e.g., "1 2 3")'
        required: true
        default: '1 2 3'
      data_version:
        description: 'Eval dataset version (DVC tag)'
        required: true
        default: 'opencloudhub-readmes-rag-evaluation-v1.0.0'
      auto_promote:
        description: 'Auto-promote best prompt to production'
        type: boolean
        default: true
      image_tag:
        description: 'Docker image tag (use "latest" to auto-resolve to main-SHA, append -eval for evaluation image)'
        required: false
        default: 'latest-eval'

jobs:
  # Step 1: Resolve the Docker image to use
  resolve-image:
    name: ðŸ” Resolve Docker Image
    runs-on: ubuntu-latest
    outputs:
      resolved_tag: ${{ steps.resolve.outputs.resolved_tag }}
      git_sha: ${{ steps.resolve.outputs.git_sha }}
      full_image: ${{ steps.resolve.outputs.full_image }}
    steps:
      - name: Resolve Docker image tag
        id: resolve
        run: |
          INPUT_TAG="${{ github.event.inputs.image_tag }}"
          DOCKER_REPO="${{ secrets.DOCKER_USERNAME }}/demo-app-genai-backend"
          
          echo "ðŸ” Resolving evaluation image tag: ${INPUT_TAG}"
          
          # Get Docker Hub token
          TOKEN=$(curl -s "https://auth.docker.io/token?service=registry.docker.io&scope=repository:${DOCKER_REPO}:pull" \
            | jq -r '.token')
          
          if [ "$INPUT_TAG" = "latest-eval" ] || [ "$INPUT_TAG" = "latest" ]; then
            echo "ðŸ” Finding latest main-*-eval image..."
            
            # Check if latest-eval exists (accept both manifest types)
            HTTP_CODE=$(curl -s -o /tmp/manifest.json -w "%{http_code}" \
              -H "Authorization: Bearer ${TOKEN}" \
              -H "Accept: application/vnd.oci.image.index.v1+json, application/vnd.docker.distribution.manifest.list.v2+json, application/vnd.docker.distribution.manifest.v2+json" \
              "https://registry-1.docker.io/v2/${DOCKER_REPO}/manifests/latest-eval")
            
            if [ "$HTTP_CODE" != "200" ]; then
              echo "âŒ Could not find 'latest-eval' tag (HTTP ${HTTP_CODE})"
              cat /tmp/manifest.json
              exit 1
            fi
            
            echo "âœ… Found 'latest-eval' tag"
            
            # Get all tags and find the latest main-*-eval
            ALL_TAGS=$(curl -s \
              -H "Authorization: Bearer ${TOKEN}" \
              "https://registry-1.docker.io/v2/${DOCKER_REPO}/tags/list" \
              | jq -r '.tags[]' 2>/dev/null | grep "^main-.*-eval$" | sort -r || echo "")
            
            echo "ðŸ“¦ Available main-*-eval tags:"
            echo "$ALL_TAGS" | head -5
            
            if [ -z "$ALL_TAGS" ]; then
              echo "âš ï¸ No main-*-eval tags found, using latest-eval directly"
              RESOLVED_TAG="latest-eval"
            else
              # Use the most recent one (sorted reverse alphabetically)
              RESOLVED_TAG=$(echo "$ALL_TAGS" | head -1)
              echo "âœ… Using most recent tag: ${RESOLVED_TAG}"
            fi
          else
            # User specified explicit tag
            if [[ "$INPUT_TAG" != *-eval ]]; then
              RESOLVED_TAG="${INPUT_TAG}-eval"
            else
              RESOLVED_TAG="$INPUT_TAG"
            fi
            
            echo "âœ… Using explicit tag: ${RESOLVED_TAG}"
            
            # Validate tag exists
            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
              -H "Authorization: Bearer ${TOKEN}" \
              -H "Accept: application/vnd.oci.image.index.v1+json, application/vnd.docker.distribution.manifest.list.v2+json, application/vnd.docker.distribution.manifest.v2+json" \
              "https://registry-1.docker.io/v2/${DOCKER_REPO}/manifests/${RESOLVED_TAG}")
            
            if [ "$HTTP_CODE" != "200" ]; then
              echo "âŒ Evaluation image not found: ${DOCKER_REPO}:${RESOLVED_TAG}"
              exit 1
            fi
          fi
          
          # Extract Git SHA from tag (format: main-abc123f-eval)
          GIT_SHA=$(echo "$RESOLVED_TAG" | sed 's/-eval$//' | sed 's/^main-//' || echo "unknown")
          
          FULL_IMAGE="${DOCKER_REPO}:${RESOLVED_TAG}"
          
          echo ""
          echo "âœ… Resolved evaluation image: ${FULL_IMAGE}"
          echo "ðŸ“ Git SHA: ${GIT_SHA}"
          
          echo "resolved_tag=${RESOLVED_TAG}" >> $GITHUB_OUTPUT
          echo "git_sha=${GIT_SHA}" >> $GITHUB_OUTPUT
          echo "full_image=${FULL_IMAGE}" >> $GITHUB_OUTPUT

  # Step 2: Run evaluation in the resolved Docker container
  evaluate:
    name: ðŸ§ª Evaluate Prompts
    needs: resolve-image
    runs-on: ubuntu-latest
    container:
      image: ${{ needs.resolve-image.outputs.full_image }}
      options: --workdir /workspace/project  # Force container workdir
      credentials:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_TOKEN }}
    
    outputs:
      best_prompt_version: ${{ steps.eval.outputs.best_prompt_version }}
    
    steps:
      - name: Verify environment
        run: |
          echo "ðŸ“ Working directory: $(pwd)"
          echo "ðŸ“‚ Contents:"
          ls -la
          echo "ðŸ Python location: $(which python)"
          echo "ðŸ“¦ DVC available: $(python -c 'import dvc; print(dvc.__version__)' 2>/dev/null || echo 'NOT FOUND')"

      - name: Run MLflow Evaluation
        id: eval
        env:
          DB_CONNECTION_STRING: ${{ secrets.DEMO_APP_DB_URL }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_ENDPOINT_URL: ${{ secrets.AWS_ENDPOINT_URL }}
          MLFLOW_TRACKING_URI: "https://mlflow.internal.opencloudhub.org"
          MLFLOW_TRACKING_INSECURE_TLS: "true"
          LLM_BASE_URL: "https://api.opencloudhub.org/models/qwen-0.5b/v1"
          DOCKER_IMAGE_TAG: ${{ needs.resolve-image.outputs.resolved_tag }}
          GIT_SHA: ${{ needs.resolve-image.outputs.git_sha }}
          GITHUB_WORKFLOW: ${{ github.workflow }}
          GITHUB_RUN_ID: ${{ github.run_id }}
          GITHUB_RUN_NUMBER: ${{ github.run_number }}
        run: |
          echo "ðŸš€ Starting evaluation with:"
          echo "  Prompt: ${{ github.event.inputs.prompt_name }}"
          echo "  Versions: ${{ github.event.inputs.prompt_versions }}"
          echo "  Data version: ${{ github.event.inputs.data_version }}"
          echo "  Docker image: ${DOCKER_IMAGE_TAG}"
          echo "  Git SHA: ${GIT_SHA}"
          echo ""
          
          # Run from the baked-in code
          python src/evaluation/evaluate_promts.py \
            --prompt-name "${{ github.event.inputs.prompt_name }}" \
            --prompt-versions ${{ github.event.inputs.prompt_versions }} \
            --data-version "${{ github.event.inputs.data_version }}" \
            ${{ github.event.inputs.auto_promote == 'true' && '--auto-promote' || '' }} \
            | tee /tmp/eval_output.txt
          
          BEST_VERSION=$(grep "Best prompt version:" /tmp/eval_output.txt | grep -oP 'v\K\d+' | head -1)
          
          if [ -z "$BEST_VERSION" ]; then
            echo "âŒ Could not extract best prompt version"
            cat /tmp/eval_output.txt
            exit 1
          fi
          
          echo ""
          echo "âœ… Evaluation complete!"
          echo "ðŸ“Š Best prompt version: ${BEST_VERSION}"
          echo "ðŸ³ Evaluated using: ${DOCKER_IMAGE_TAG} (${GIT_SHA})"
          
          echo "best_prompt_version=${BEST_VERSION}" >> $GITHUB_OUTPUT

      - name: Upload evaluation logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-logs-${{ github.run_number }}
          path: /tmp/eval_output.txt
          retention-days: 30

  # Step 3: Trigger API reload with new prompt
  reload-api:
    name: ðŸ”„ Reload Production API
    needs: [resolve-image, evaluate]
    if: github.event.inputs.auto_promote == 'true' && needs.evaluate.outputs.best_prompt_version != ''
    runs-on: ubuntu-latest
    
    steps:
      - name: Trigger API reload
        env:
          API_URL: "https://demo-app.opencloudhub.org/api/admin/reload-prompt"
          BEST_PROMPT_VERSION: ${{ needs.evaluate.outputs.best_prompt_version }}
        run: |
          echo "ðŸ”„ Reloading API with new prompt..."
          echo "  API: ${API_URL}"
          echo "  Prompt version: ${BEST_PROMPT_VERSION}"
          
          RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
            "${API_URL}/admin/reload-prompt" \
            -H "Content-Type: application/json" \
            -d "{\"prompt_version\":${BEST_PROMPT_VERSION}}")
          
          HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
          BODY=$(echo "$RESPONSE" | head -n-1)
          
          echo "Response: ${BODY}"
          echo "Status: ${HTTP_CODE}"
          
          if [ "$HTTP_CODE" = "200" ]; then
            echo "âœ… API successfully reloaded with prompt v${BEST_PROMPT_VERSION}"
          else
            echo "âš ï¸ API reload returned status ${HTTP_CODE}"
            echo "Response: ${BODY}"
            exit 1
          fi

  # Summary job
  summary:
    name: ðŸ“‹ Summary
    needs: [resolve-image, evaluate, reload-api]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
      - name: Create Job Summary
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          ## ðŸŽ¯ RAG Prompt Evaluation Results
          
          ### Configuration
          | Parameter | Value |
          |-----------|-------|
          | **Prompt Name** | `${{ github.event.inputs.prompt_name }}` |
          | **Versions Evaluated** | `${{ github.event.inputs.prompt_versions }}` |
          | **Dataset Version** | `${{ github.event.inputs.data_version }}` |
          | **Auto-Promote** | ${{ github.event.inputs.auto_promote }} |
          
          ### Docker Image
          | Property | Value |
          |----------|-------|
          | **Input Tag** | `${{ github.event.inputs.image_tag }}` |
          | **Resolved Tag** | `${{ needs.resolve-image.outputs.resolved_tag }}` |
          | **Git SHA** | `${{ needs.resolve-image.outputs.git_sha }}` |
          | **Full Image** | `${{ needs.resolve-image.outputs.full_image }}` |
          
          ### Results
          | Metric | Value |
          |--------|-------|
          | **Best Prompt Version** | `v${{ needs.evaluate.outputs.best_prompt_version }}` |
          | **Evaluation Status** | ${{ needs.evaluate.result == 'success' && 'âœ… Success' || 'âŒ Failed' }} |
          | **API Reload Status** | ${{ needs.reload-api.result == 'success' && 'âœ… Success' || needs.reload-api.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |
          
          ### Next Steps
          ${{ github.event.inputs.auto_promote == 'true' && format('
          âœ… The best prompt (v{0}) has been automatically promoted to `@production` and the API has been reloaded.
          
          **Your production API is now using:**
          ```python
          prompt_name: "readme-rag-prompt"
          prompt_version: {0}
          evaluated_with_image: "{1}"
          evaluated_with_code: "{2}"
          ```
          ', needs.evaluate.outputs.best_prompt_version, needs.resolve-image.outputs.resolved_tag, needs.resolve-image.outputs.git_sha) || format('
          â„¹ï¸ Evaluation complete but auto-promotion was disabled.
          
          **To manually promote:**
          ```python
          import mlflow
          mlflow.set_prompt_alias(
              "readme-rag-prompt", 
              alias="production", 
              version={0}
          )
          ```
          ', needs.evaluate.outputs.best_prompt_version) }}
          
          ---
          
          ðŸ“Š [View detailed results in MLflow](${{ secrets.MLFLOW_TRACKING_URI }})  
          ðŸ“¦ [Download evaluation logs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          EOF