<a id="readme-top"></a>

<!-- PROJECT LOGO & TITLE -->

<div align="center">
  <a href="https://github.com/opencloudhub">
  <picture>
    <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/opencloudhub/.github/main/assets/brand/assets/logos/primary-logo-light.svg">
    <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/opencloudhub/.github/main/assets/brand/assets/logos/primary-logo-dark.svg">
    <!-- Fallback -->
    <img alt="OpenCloudHub Logo" src="https://raw.githubusercontent.com/opencloudhub/.github/main/assets/brand/assets/logos/primary-logo-dark.svg" style="max-width:700px; max-height:175px;">
  </picture>
  </a>

<h1 align="center">Demo App GenAI Backend</h1>

<p align="center">
    Demo RAG system with prompt versioning, automated evaluation, and GitOps deployment.<br />
    <a href="https://github.com/opencloudhub"><strong>Explore OpenCloudHub Â»</strong></a>
  </p>
</div>

______________________________________________________________________

<details>
  <summary>ğŸ“‘ Table of Contents</summary>
  <ol>
    <li><a href="#about">About</a></li>
    <li><a href="#thesis-context">Thesis Context</a></li>
    <li><a href="#features">Features</a></li>
    <li><a href="#architecture">Architecture</a></li>
    <li><a href="#getting-started">Getting Started</a></li>
    <li><a href="#configuration">Configuration</a></li>
    <li><a href="#mlops-workflow">MLOps Workflow</a></li>
    <li><a href="#project-structure">Project Structure</a></li>
    <li><a href="#contributing">Contributing</a></li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
  </ol>
</details>

______________________________________________________________________

<h2 id="about">ğŸ¯ About</h2>

A production-ready Retrieval-Augmented Generation (RAG) system that answers questions about OpenCloudHub's MLOps platform by retrieving relevant information from README files across multiple repositories.

The system demonstrates **MLOps best practices in the context of GenAI applications**, including:

- ğŸ” **Semantic search** over OpenCloudHub repository READMEs
- ğŸ’¬ **Context-aware question answering** with streaming responses and chat history
- ğŸ“Š **Automated prompt evaluation** with LLM-as-judge scoring
- ğŸš€ **GitOps-based deployment** with ArgoCD Image Updater
- ğŸ¯ **MLflow-based experiment tracking** and prompt registry

______________________________________________________________________

<h2 id="thesis-context">ğŸ“š Thesis Context</h2>

This repository is part of the **OpenCloudHub MLOps platform demonstration**, showcasing how to operationalize GenAI applications with proper MLOps practices:

| Aspect                | Implementation                                                   |
| --------------------- | ---------------------------------------------------------------- |
| **Prompt Versioning** | MLflow Prompt Registry with semantic aliases (`@champion`)       |
| **Evaluation**        | Automated scoring with custom metrics + LLM-as-judge             |
| **Data Versioning**   | DVC-tracked evaluation datasets                                  |
| **Deployment**        | GitOps via ArgoCD with automatic image updates                   |
| **Observability**     | OpenTelemetry tracing â†’ Tempo, Prometheus metrics                |
| **Self-Hosted LLM**   | Integration with Qwen model served via Ray Serve on the platform |

______________________________________________________________________

<h2 id="features">âœ¨ Features</h2>

### RAG System

- **Hybrid Search**: Combines semantic (pgvector) and keyword (PostgreSQL FTS) search with reciprocal rank fusion
- **Streaming Responses**: Server-Sent Events (SSE) for real-time token streaming
- **Session Management**: PostgreSQL-backed chat history across multiple queries
- **Production-Ready Serving**: FastAPI with health checks, metrics, and graceful shutdown

### MLOps Pipeline

- **Prompt Versioning**: MLflow Prompt Registry with `@champion` alias for production
- **Automated Evaluation**: Compare prompt versions using custom scorers and LLM-as-judge
- **Auto-Promotion**: Best-performing prompts automatically promoted to production
- **Data Versioning**: DVC-tracked evaluation datasets with lineage
- **CI/CD Integration**: GitHub Actions for quality checks and Docker builds
- **GitOps Deployment**: ArgoCD with automatic image updates on promotion

### Observability

- **Distributed Tracing**: OpenTelemetry â†’ Tempo for end-to-end request tracing
- **Prometheus Metrics**: Request counts, latency histograms, with trace exemplars
- **Log Correlation**: Automatic trace_id/span_id injection into logs
- **Experiment Tracking**: MLflow for prompt evaluation metrics and comparison

______________________________________________________________________

<h2 id="architecture">ğŸ—ï¸ Architecture</h2>

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Prompt Evaluation Pipeline                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Prompt     â”‚ prompt v1, v2, v3...   â”‚  Evaluation  â”‚       â”‚
â”‚  â”‚  Registry    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   Dataset    â”‚       â”‚
â”‚  â”‚  (MLflow)    â”‚          â”‚             â”‚    (DVC)     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                            â”‚                     â”‚              â”‚
â”‚                            â–¼                     â–¼              â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                      â”‚     Automated Evaluation      â”‚          â”‚
â”‚                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚          â”‚
â”‚                      â”‚  â”‚ For each prompt version â”‚  â”‚          â”‚
â”‚                      â”‚  â”‚ + each Q&A pair:        â”‚  â”‚          â”‚
â”‚                      â”‚  â”‚ â€¢ Run RAG pipeline      â”‚  â”‚          â”‚
â”‚                      â”‚  â”‚ â€¢ Score vs expected     â”‚  â”‚          â”‚
â”‚                      â”‚  â”‚ â€¢ LLM-as-judge quality  â”‚  â”‚          â”‚
â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                     â”‚                           â”‚
â”‚                                     â–¼                           â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                            â”‚   Promote    â”‚                     â”‚
â”‚                            â”‚  @champion   â”‚                     â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Production Runtime                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚   FastAPI    â”‚â”€â”€â”€â–¶â”‚  RAG Chain   â”‚â”€â”€â”€â–¶â”‚  PostgreSQL  â”‚    â”‚
â”‚   â”‚   + OTEL     â”‚     â”‚  (LangChain) â”‚     â”‚  + pgvector  â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚          â”‚                     â”‚                     â”‚          â”‚
â”‚          â–¼                     â–¼                     â–¼          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚    Tempo     â”‚     â”‚  Qwen LLM    â”‚     â”‚ Chat History â”‚    â”‚
â”‚   â”‚   (Traces)   â”‚     â”‚  (KServe)    â”‚     â”‚  (Postgres)  â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GitOps Deployment (ArgoCD)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   GitHub Actions â†’ Docker Build â†’ Registry â†’ ArgoCD â†’ K8s       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Query**: User question â†’ Hybrid retrieval (semantic + keyword) â†’ Context + Prompt â†’ LLM â†’ Streamed answer
1. **Evaluation**:
   - Load prompt versions from MLflow (v1, v2, v3...)
   - Load Q&A dataset from DVC (`question`, `expected_answer`, `key_concepts`, `category`)
   - For each prompt Ã— each question: run RAG, score answer vs expected
   - Promote best-scoring prompt to `@champion`
1. **Deployment**: GitHub Actions â†’ Docker build â†’ Push to registry â†’ ArgoCD watches â†’ Rolling update

______________________________________________________________________

<h2 id="getting-started">ğŸš€ Getting Started</h2>

### Prerequisites

- Docker & Docker Compose
- VS Code with DevContainers extension (recommended)
- Access to:
  - PostgreSQL database with pgvector extension
  - OpenAI-compatible LLM endpoint
  - MLflow tracking server
  - MinIO/S3 for DVC storage

### Quick Start

1. **Clone the repository**

   ```bash
   git clone https://github.com/OpenCloudHub/demo-app-genai-backend.git
   cd demo-app-genai-backend
   ```

1. **Open in DevContainer**

   VSCode: `Ctrl+Shift+P` â†’ `Dev Containers: Rebuild and Reopen in Container`

1. **Configure environment**

   ```bash
   # For minikube deployment (with port-forwarding)
   cp .env.minikube .env

   # Apply environment variables
   set -a && source .env && set +a
   ```

1. **Port-forward required services** (if connecting to minikube from local repo)

   ```bash
   # PostgreSQL
   kubectl port-forward -n storage svc/demo-app-db-cluster-rw 5432:5432 &

   # OTEL collector (Alloy)
   kubectl port-forward -n observability svc/k8s-monitoring-alloy-receiver 4317:4317 &
   ```

1. **Run the API**

   ```bash
   # Development mode with auto-reload
   fastapi dev src/main.py

   # Production mode
   uvicorn src.main:app --host 0.0.0.0 --port 8000
   ```

1. **Test the API**

   ```bash
   # Simple query
   curl -X POST http://localhost:8000/api/query \
     -H "Content-Type: application/json" \
     -d '{"question": "What is GitOps in OpenCloudHub?"}'

   # Streaming with Server-Sent Events
   curl -N -X POST http://localhost:8000/api/query \
     -H "Content-Type: application/json" \
     -H "Accept: text/event-stream" \
     -d '{"question": "What is GitOps in OpenCloudHub?", "stream": true}'
   ```

______________________________________________________________________

<h2 id="configuration">âš™ï¸ Configuration</h2>

### Environment Variables

All configuration is managed via environment variables (Pydantic Settings):

| Variable                      | Description                  | Default             |
| ----------------------------- | ---------------------------- | ------------------- |
| `DB_HOST`                     | PostgreSQL host              | `127.0.0.1`         |
| `DB_PORT`                     | PostgreSQL port              | `5432`              |
| `DB_NAME`                     | Database name                | `demo_app`          |
| `DB_USER`                     | Database user                | *required*          |
| `DB_PASSWORD`                 | Database password            | *required*          |
| `DB_TABLE_NAME`               | Embeddings table             | `readme_embeddings` |
| `DB_TOP_K`                    | Number of docs to retrieve   | `10`                |
| `MLFLOW_TRACKING_URI`         | MLflow server URL            | *required*          |
| `PROMPT_NAME`                 | Prompt name in registry      | `readme-rag-prompt` |
| `LLM_BASE_URL`                | OpenAI-compatible API URL    | *required*          |
| `LLM_MODEL`                   | Model name                   | *required*          |
| `EMBEDDING_MODEL`             | Sentence transformer model   | *required*          |
| `OTEL_ENABLED`                | Enable OpenTelemetry tracing | `true`              |
| `OTEL_EXPORTER_OTLP_ENDPOINT` | OTLP gRPC endpoint           | `localhost:4317`    |

### API Endpoints

| Endpoint                    | Method | Description                         |
| --------------------------- | ------ | ----------------------------------- |
| `/api/`                     | GET    | Service info                        |
| `/api/health`               | GET    | Health check with prompt version    |
| `/api/query`                | POST   | Ask a question (supports streaming) |
| `/api/session/create`       | POST   | Create a new chat session           |
| `/api/session/{id}/history` | GET    | Get session chat history            |
| `/api/session/{id}`         | DELETE | Clear session history               |
| `/api/admin/reload-prompt`  | POST   | Hot-reload prompt version           |
| `/api/debug/retrieval`      | POST   | Debug document retrieval            |
| `/metrics`                  | GET    | Prometheus metrics                  |

______________________________________________________________________

<h2 id="mlops-workflow">ğŸ”„ MLOps Workflow</h2>

### 1. Prompt Development

Register new prompt versions in MLflow:

```bash
python src/prompts/register_prompts.py
```

This registers multiple prompt versions (V1: baseline, V2: medium, V3: optimized) for A/B testing.

### 2. Automated Evaluation

Run evaluation to compare prompt versions:

```bash
# Local execution
python src/prompts/evaluate_promts.py \
    --prompt-name readme-rag-prompt \
    --prompt-versions 1 2 3 \
    --dvc-data-version opencloudhub-readmes-rag-evaluation-v1.0.0 \
    --auto-promote

# Via GitHub Actions
gh workflow run evaluate-and-promote-rag.yaml \
    -f prompt_name=readme-rag-prompt \
    -f prompt_versions="1 2 3" \
    -f data_version=opencloudhub-readmes-rag-evaluation-v1.0.0
```

**Evaluation Process:**

1. Loads evaluation dataset from DVC (versioned questions + expected answers)
1. Runs each prompt version through the RAG pipeline
1. Computes metrics:
   - **Concept Coverage**: Checks if key concepts are mentioned in the answer
   - **LLM-as-Judge**: Quality assessment using the same LLM
1. Calculates composite score (60% concept + 40% judge)
1. Promotes best prompt to `@champion` alias

### 3. Hot Reload in Production

After promotion, reload the running API without restart:

```bash
curl -X POST http://localhost:8000/api/admin/reload-prompt \
  -H "Content-Type: application/json" \
  -d '{"prompt_version": null, "top_k": 5}'  # null loads @champion
```

### 4. Continuous Deployment

```mermaid
graph LR
    A[Register Prompt] --> B[Evaluate Versions]
    B --> C[Promote to @champion]
    C --> D[GitHub Actions Build]
    D --> E[Push to Registry]
    E --> F[ArgoCD Image Updater]
    F --> G[Rolling Update]
```

______________________________________________________________________

<h2 id="project-structure">ğŸ“ Project Structure</h2>

```
demo-app-genai-backend/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci-code-quality.yaml          # Ruff linting, type checks
â”‚       â”œâ”€â”€ ci-docker-build-push.yaml     # Multi-stage Docker builds
â”‚       â””â”€â”€ evaluate-and-promote-rag.yaml # Automated evaluation pipeline
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                           # FastAPI application entry point
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ deps.py                       # FastAPI dependencies
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ admin.py                  # Prompt reload endpoint
â”‚   â”‚       â”œâ”€â”€ debug.py                  # Debug/retrieval testing
â”‚   â”‚       â”œâ”€â”€ health.py                 # Health checks
â”‚   â”‚       â”œâ”€â”€ query.py                  # Main query endpoint
â”‚   â”‚       â””â”€â”€ session.py                # Chat session management
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py                     # Pydantic settings
â”‚   â”‚   â”œâ”€â”€ database.py                   # Database connection manager
â”‚   â”‚   â”œâ”€â”€ logging.py                    # Loguru configuration
â”‚   â”‚   â””â”€â”€ tracing.py                    # OpenTelemetry + Prometheus
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ evaluate_promts.py            # MLflow GenAI evaluation
â”‚   â”‚   â””â”€â”€ register_prompts.py           # Prompt registration
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ chain.py                      # RAGChain with LangChain
â”‚   â”‚   â””â”€â”€ embeddings.py                 # SentenceTransformer wrapper
â”‚   â””â”€â”€ schemas/                          # Pydantic request/response models
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ debug_retrieval.py                # Test retrieval quality
â”‚   â””â”€â”€ test_streaming_curl.sh            # Test SSE streaming
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_streaming.py                 # Streaming response tests
â”‚
â”œâ”€â”€ .env.minikube                         # Minikube environment template
â”œâ”€â”€ Dockerfile                            # Multi-stage production build
â”œâ”€â”€ pyproject.toml                        # Python dependencies (uv)
â””â”€â”€ README.md
```

______________________________________________________________________

<h2 id="contributing">ğŸ‘¥ Contributing</h2>

Contributions are welcome! This project follows OpenCloudHub's contribution standards.

Please see our [Contributing Guidelines](https://github.com/opencloudhub/.github/blob/main/.github/CONTRIBUTING.md) and [Code of Conduct](https://github.com/opencloudhub/.github/blob/main/.github/CODE_OF_CONDUCT.md).

**Development workflow:**

1. Fork the repository
1. Create a feature branch (`git checkout -b feature/amazing-feature`)
1. Make changes with tests
1. Run quality checks (`ruff check . && ruff format .`)
1. Commit (`git commit -m 'Add amazing feature'`)
1. Push to branch (`git push origin feature/amazing-feature`)
1. Open a Pull Request

______________________________________________________________________

<h2 id="license">ğŸ“„ License</h2>

Distributed under the Apache 2.0 License. See [LICENSE](LICENSE) for more information.

______________________________________________________________________

<h2 id="contact">ğŸ“¬ Contact</h2>

**Organization:** [OpenCloudHub](https://github.com/OpenCloudHub)

**Project:** [demo-app-genai-backend](https://github.com/OpenCloudHub/demo-app-genai-backend)

______________________________________________________________________

<h2 id="acknowledgements">ğŸ™ Acknowledgements</h2>

- [LangChain](https://python.langchain.com/) - RAG orchestration framework
- [FastAPI](https://fastapi.tiangolo.com/) - Modern async web framework
- [MLflow](https://mlflow.org/) - ML lifecycle management & prompt registry
- [DVC](https://dvc.org/) - Data version control
- [pgvector](https://github.com/pgvector/pgvector) - Vector similarity search for PostgreSQL
- [OpenTelemetry](https://opentelemetry.io/) - Observability framework
- [ArgoCD](https://argo-cd.readthedocs.io/) - GitOps continuous delivery

<p align="right">(<a href="#readme-top">back to top</a>)</p>

______________________________________________________________________

<div align="center">
  <h3>ğŸŒŸ Follow the Journey</h3>
  <p><em>Building in public â€¢ Learning together â€¢ Sharing knowledge</em></p>

<div>
    <a href="https://opencloudhub.github.io/docs">
      <img src="https://img.shields.io/badge/Read%20the%20Docs-2596BE?style=for-the-badge&logo=read-the-docs&logoColor=white" alt="Documentation">
    </a>
    <a href="https://github.com/orgs/opencloudhub/discussions">
      <img src="https://img.shields.io/badge/Join%20Discussion-181717?style=for-the-badge&logo=github&logoColor=white" alt="Discussions">
    </a>
    <a href="https://github.com/orgs/opencloudhub/projects/4">
      <img src="https://img.shields.io/badge/View%20Roadmap-0052CC?style=for-the-badge&logo=jira&logoColor=white" alt="Roadmap">
    </a>
  </div>
</div>
